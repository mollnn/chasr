{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import codecs\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13388, 680, 26])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = \"./dataset/data_thchs30\"\n",
    "mfcc_mat = np.load(os.path.join(dataset_path, \"mfcc_vec_680x26.npy\"))\n",
    "mfcc_mat = torch.tensor(mfcc_mat)\n",
    "mfcc_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13388,\n",
       " ['绿是阳春烟景大块文章的底色四月的林峦更是绿得鲜活秀媚诗意盎然\\n',\n",
       "  '他仅凭腰部的力量在泳道上下翻腾蛹动蛇行状如海豚一直以一头的优势领先\\n',\n",
       "  '炮眼打好了炸药怎么装岳正才咬了咬牙倏地脱去衣服光膀子冲进了水窜洞\\n'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with codecs.open(os.path.join(dataset_path, \"all_texts.txt\"), encoding=\"utf-8\") as file_read:\n",
    "    text_lines = file_read.readlines()\n",
    "len(text_lines), text_lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13388, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_set = set(list(''.join(text_lines).replace(\"\\n\",\"\")))\n",
    "token_map = dict((j,i+1) for i,j in enumerate(token_set))\n",
    "print(len(token_map))\n",
    "seq_lines = [list(map(lambda x: token_map[x], text_line.replace(\"\\n\",\"\"))) for text_line in text_lines]\n",
    "len(seq_lines), len(seq_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_lines = [(seq_line + [0]*48)[:48] for seq_line in seq_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        dataset_path = \"./dataset/data_thchs30\"\n",
    "        self.mfcc_mat = np.load(os.path.join(dataset_path, \"mfcc_vec_680x26.npy\"))\n",
    "        with codecs.open(os.path.join(dataset_path, \"all_texts.txt\"), encoding=\"utf-8\") as file_read:\n",
    "            text_lines = file_read.readlines()\n",
    "        token_set = set(list(''.join(text_lines).replace(\"\\n\",\"\")))\n",
    "        token_map = dict((j,i+1) for i,j in enumerate(token_set))\n",
    "        seq_lines = [list(map(lambda x: token_map[x], text_line.replace(\"\\n\",\"\"))) for text_line in text_lines]\n",
    "        self.pad_lines = [(seq_line + [0]*48)[:48] for seq_line in seq_lines]\n",
    "        self.pad_lines = torch.tensor(self.pad_lines).unsqueeze(-1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.mfcc_mat)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.mfcc_mat[idx], self.pad_lines[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyDataset at 0x28de0ff25c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = MyDataset()\n",
    "my_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torch.utils.data.random_split(my_dataset, [13000, 388])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.stage = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(26, 50, kernel_size=5, stride=1, padding=2),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Conv1d(50, 2884, kernel_size=5, stride=1, padding=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.stage(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2884, 680])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([680, 4, 2884])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MyModel()\n",
    "tmp = torch.randn((4,26,680))\n",
    "print(net(tmp).shape)\n",
    "torch.transpose(torch.transpose(net(tmp), 0, 2), 1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(153.8136, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is for demo\n",
    "batch_size = 8\n",
    "ctc_loss = torch.nn.CTCLoss()\n",
    "log_probs = torch.randn(680, batch_size, 2884).log_softmax(2).detach().requires_grad_()\n",
    "targets = torch.randint(1, 2884, (batch_size, 48), dtype=torch.long)\n",
    "input_lengths = torch.full((batch_size,), 680, dtype=torch.long)\n",
    "target_lengths = torch.randint(24, 48, (batch_size,), dtype=torch.long)\n",
    "loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = torch.utils.data.DataLoader(data_train, batch_size = batch_size, shuffle=True)\n",
    "dataloader_test = torch.utils.data.DataLoader(data_test, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0944e+01, -4.1253e+01,  6.7133e-01,  ..., -1.1126e-01,\n",
      "           3.9241e-01,  1.4940e-01],\n",
      "         [ 2.0943e+01, -4.1742e+01,  3.5420e-01,  ..., -9.3708e-02,\n",
      "           4.7631e-01,  5.7791e-01],\n",
      "         [ 1.3963e+01, -2.5509e+01, -1.0801e+01,  ..., -5.8580e-02,\n",
      "           4.5159e-01, -8.9573e-02],\n",
      "         ...,\n",
      "         [ 1.7983e+01,  3.9722e+00, -1.8970e+01,  ..., -4.4753e-01,\n",
      "           1.1611e+00,  2.9819e+00],\n",
      "         [ 1.8745e+01,  3.0303e+00, -2.1659e+01,  ..., -5.2191e-01,\n",
      "           3.2556e+00,  4.6692e+00],\n",
      "         [ 1.9063e+01,  1.8797e+00, -2.2774e+01,  ..., -6.0087e-01,\n",
      "           3.0301e+00,  5.0095e+00]],\n",
      "\n",
      "        [[ 1.1203e+01, -8.8239e+00, -1.2761e+01,  ..., -4.1323e-01,\n",
      "           2.8512e-01, -7.1445e-01],\n",
      "         [ 1.0919e+01, -7.9935e+00, -1.2961e+01,  ..., -3.7978e-01,\n",
      "          -1.0755e-01, -1.8388e+00],\n",
      "         [ 1.0632e+01, -8.7431e+00, -1.2541e+01,  ..., -4.3447e-01,\n",
      "          -6.0046e-01, -6.2409e-01],\n",
      "         ...,\n",
      "         [ 1.5245e+01,  2.2344e+01, -4.7304e+00,  ..., -4.4450e-01,\n",
      "          -3.1446e-01, -1.8623e+00],\n",
      "         [ 1.4920e+01,  1.9279e+01, -6.0866e+00,  ..., -4.7422e-01,\n",
      "          -1.3158e+00, -1.9085e+00],\n",
      "         [ 1.4795e+01,  1.8387e+01, -6.9536e+00,  ..., -6.1422e-01,\n",
      "          -2.2648e+00, -2.1570e+00]],\n",
      "\n",
      "        [[ 1.2291e+01, -1.5053e+01, -2.5584e+01,  ..., -2.4142e-01,\n",
      "           1.2653e-01,  1.3622e+00],\n",
      "         [ 1.2485e+01, -1.3313e+01, -2.8268e+01,  ..., -9.9750e-02,\n",
      "           4.1183e-01,  8.6636e-01],\n",
      "         [ 1.2752e+01, -1.4101e+01, -2.9135e+01,  ..., -1.1527e-02,\n",
      "           2.8460e-01,  1.5120e+00],\n",
      "         ...,\n",
      "         [ 1.9836e+01, -1.5235e+01, -8.4297e+00,  ..., -9.4570e-01,\n",
      "          -2.7480e+00, -4.9143e+00],\n",
      "         [ 1.9928e+01, -1.3640e+01, -1.2573e+01,  ..., -9.8779e-01,\n",
      "          -2.7731e+00, -4.0995e+00],\n",
      "         [ 2.0154e+01, -1.3955e+01, -1.4548e+01,  ..., -1.0633e+00,\n",
      "          -3.0853e+00, -4.9506e+00]],\n",
      "\n",
      "        [[ 1.1831e+01, -8.4550e+00,  1.0473e+01,  ...,  1.9545e-02,\n",
      "           2.8299e-02, -3.5043e-02],\n",
      "         [ 1.1281e+01, -1.2718e+01,  6.8325e+00,  ..., -1.3863e-01,\n",
      "           3.6937e-03,  1.2856e+00],\n",
      "         [ 1.1242e+01, -1.2866e+01,  6.6605e+00,  ..., -9.8397e-02,\n",
      "          -3.4721e-01, -1.8954e-01],\n",
      "         ...,\n",
      "         [ 1.9502e+01, -2.4645e+01,  1.4992e+01,  ...,  1.5900e-02,\n",
      "          -9.4450e-02,  6.7779e-01],\n",
      "         [ 2.0215e+01, -2.5013e+01, -8.7407e+00,  ..., -1.6234e-01,\n",
      "           3.8238e-02,  1.2886e-01],\n",
      "         [ 2.1172e+01, -2.4989e+01, -1.8343e+01,  ..., -1.0313e-01,\n",
      "          -1.6558e+00, -1.2180e+00]]], dtype=torch.float64) tensor([[[1503],\n",
      "         [1733],\n",
      "         [2186],\n",
      "         [ 280],\n",
      "         [ 467],\n",
      "         [2488],\n",
      "         [ 904],\n",
      "         [ 273],\n",
      "         [2023],\n",
      "         [1634],\n",
      "         [1892],\n",
      "         [2202],\n",
      "         [1415],\n",
      "         [2042],\n",
      "         [2332],\n",
      "         [ 140],\n",
      "         [1248],\n",
      "         [2781],\n",
      "         [1801],\n",
      "         [1474],\n",
      "         [2142],\n",
      "         [1834],\n",
      "         [2173],\n",
      "         [2439],\n",
      "         [  68],\n",
      "         [1801],\n",
      "         [1708],\n",
      "         [2403],\n",
      "         [2403],\n",
      "         [2320],\n",
      "         [2320],\n",
      "         [ 658],\n",
      "         [2106],\n",
      "         [ 257],\n",
      "         [1706],\n",
      "         [2202],\n",
      "         [ 473],\n",
      "         [2706],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0]],\n",
      "\n",
      "        [[ 771],\n",
      "         [1549],\n",
      "         [1365],\n",
      "         [1311],\n",
      "         [1193],\n",
      "         [2262],\n",
      "         [2288],\n",
      "         [1254],\n",
      "         [1777],\n",
      "         [1181],\n",
      "         [ 177],\n",
      "         [1754],\n",
      "         [ 231],\n",
      "         [ 291],\n",
      "         [1754],\n",
      "         [ 714],\n",
      "         [ 721],\n",
      "         [ 926],\n",
      "         [  63],\n",
      "         [ 335],\n",
      "         [ 379],\n",
      "         [ 843],\n",
      "         [1216],\n",
      "         [1811],\n",
      "         [ 223],\n",
      "         [1777],\n",
      "         [1181],\n",
      "         [ 952],\n",
      "         [1445],\n",
      "         [ 212],\n",
      "         [ 545],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0]],\n",
      "\n",
      "        [[1920],\n",
      "         [1985],\n",
      "         [2800],\n",
      "         [ 435],\n",
      "         [2009],\n",
      "         [  57],\n",
      "         [2425],\n",
      "         [ 804],\n",
      "         [1561],\n",
      "         [1206],\n",
      "         [1207],\n",
      "         [2780],\n",
      "         [ 373],\n",
      "         [ 368],\n",
      "         [1213],\n",
      "         [ 278],\n",
      "         [  47],\n",
      "         [1544],\n",
      "         [1297],\n",
      "         [1360],\n",
      "         [1206],\n",
      "         [ 685],\n",
      "         [2752],\n",
      "         [1360],\n",
      "         [1206],\n",
      "         [ 685],\n",
      "         [2516],\n",
      "         [2231],\n",
      "         [ 211],\n",
      "         [2839],\n",
      "         [2831],\n",
      "         [2080],\n",
      "         [1313],\n",
      "         [ 950],\n",
      "         [1206],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0]],\n",
      "\n",
      "        [[1472],\n",
      "         [1549],\n",
      "         [ 915],\n",
      "         [1083],\n",
      "         [ 202],\n",
      "         [  54],\n",
      "         [2338],\n",
      "         [2578],\n",
      "         [1497],\n",
      "         [2317],\n",
      "         [1867],\n",
      "         [2850],\n",
      "         [2613],\n",
      "         [ 869],\n",
      "         [ 380],\n",
      "         [ 325],\n",
      "         [2391],\n",
      "         [1664],\n",
      "         [ 385],\n",
      "         [1165],\n",
      "         [2557],\n",
      "         [2387],\n",
      "         [2035],\n",
      "         [ 462],\n",
      "         [2430],\n",
      "         [2426],\n",
      "         [2347],\n",
      "         [2338],\n",
      "         [1159],\n",
      "         [2035],\n",
      "         [ 529],\n",
      "         [ 375],\n",
      "         [2450],\n",
      "         [ 151],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0],\n",
      "         [   0]]])\n"
     ]
    }
   ],
   "source": [
    "for i,j in dataloader_train:\n",
    "    print(i,j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 130.65274047851562\n",
      "1 123.19671630859375\n",
      "2 128.10986328125\n",
      "3 129.64865112304688\n",
      "4 128.28305053710938\n",
      "5 120.76300048828125\n",
      "6 126.61337280273438\n",
      "7 120.30543518066406\n",
      "8 113.39134216308594\n",
      "9 126.79279327392578\n",
      "10 125.12100219726562\n",
      "11 136.17364501953125\n",
      "12 134.31430053710938\n",
      "13 132.18272399902344\n",
      "14 126.92364501953125\n",
      "15 127.32121276855469\n",
      "16 144.0885772705078\n",
      "17 124.84217834472656\n",
      "18 126.72932434082031\n",
      "19 129.77639770507812\n",
      "20 124.54583740234375\n",
      "21 126.73545837402344\n",
      "22 130.92990112304688\n",
      "23 126.32046508789062\n",
      "24 144.81375122070312\n",
      "25 131.81141662597656\n",
      "26 123.07147979736328\n",
      "27 118.42012786865234\n",
      "28 138.7265625\n",
      "29 129.5321044921875\n",
      "30 121.78547668457031\n",
      "31 130.73707580566406\n",
      "32 122.19229888916016\n",
      "33 144.44847106933594\n",
      "34 134.82272338867188\n",
      "35 134.0120849609375\n",
      "36 132.3302001953125\n",
      "37 126.5694580078125\n",
      "38 121.38441467285156\n",
      "39 132.16123962402344\n",
      "40 132.57183837890625\n",
      "41 124.88327026367188\n",
      "42 129.2086181640625\n",
      "43 132.24630737304688\n",
      "44 139.4387664794922\n",
      "45 129.31007385253906\n",
      "46 127.56576538085938\n",
      "47 122.86038970947266\n",
      "48 128.3245391845703\n",
      "49 133.91372680664062\n",
      "50 132.22848510742188\n",
      "51 126.39448547363281\n",
      "52 123.88978576660156\n",
      "53 127.38243103027344\n",
      "54 117.04901123046875\n",
      "55 144.06826782226562\n",
      "56 138.40011596679688\n",
      "57 132.99398803710938\n",
      "58 124.07806396484375\n",
      "59 128.59634399414062\n",
      "60 140.47447204589844\n",
      "61 123.35606384277344\n",
      "62 121.15116119384766\n",
      "63 127.03561401367188\n",
      "64 134.96527099609375\n",
      "65 136.26390075683594\n",
      "66 146.52035522460938\n",
      "67 139.5281219482422\n",
      "68 134.91131591796875\n",
      "69 124.974365234375\n",
      "70 131.10455322265625\n",
      "71 123.93075561523438\n",
      "72 128.748291015625\n",
      "73 126.39265441894531\n",
      "74 126.65713500976562\n",
      "75 124.30010986328125\n",
      "76 121.63184356689453\n",
      "77 111.60975646972656\n",
      "78 125.82289123535156\n",
      "79 142.823486328125\n",
      "80 119.3681869506836\n",
      "81 132.92861938476562\n",
      "82 123.27291870117188\n",
      "83 128.07330322265625\n",
      "84 126.34207153320312\n",
      "85 134.50704956054688\n",
      "86 136.13050842285156\n",
      "87 126.43647766113281\n",
      "88 123.12600708007812\n",
      "89 129.8837890625\n",
      "90 139.14085388183594\n",
      "91 115.01031494140625\n",
      "92 131.50076293945312\n",
      "93 130.23184204101562\n",
      "94 117.90434265136719\n",
      "95 119.04862213134766\n",
      "96 122.76799011230469\n",
      "97 132.67379760742188\n",
      "98 141.59629821777344\n",
      "99 122.06192779541016\n",
      "100 124.3675308227539\n",
      "101 137.54086303710938\n",
      "102 124.44063568115234\n",
      "103 126.7717056274414\n",
      "104 124.16365051269531\n",
      "105 115.03099060058594\n",
      "106 139.9971923828125\n",
      "107 128.781494140625\n",
      "108 124.43712615966797\n",
      "109 130.41236877441406\n",
      "110 132.44866943359375\n",
      "111 126.55221557617188\n",
      "112 131.19998168945312\n",
      "113 127.70561218261719\n",
      "114 136.979736328125\n",
      "115 120.51602172851562\n",
      "116 136.9529266357422\n",
      "117 133.22030639648438\n",
      "118 139.13516235351562\n",
      "119 124.09925842285156\n",
      "120 123.52534484863281\n",
      "121 123.31277465820312\n",
      "122 134.08184814453125\n",
      "123 128.03135681152344\n",
      "124 134.04354858398438\n",
      "125 123.33557891845703\n",
      "126 122.26374816894531\n",
      "127 131.65682983398438\n",
      "128 120.45291137695312\n",
      "129 122.32174682617188\n",
      "130 124.19441986083984\n",
      "131 130.6688690185547\n",
      "132 132.79409790039062\n",
      "133 120.46796417236328\n",
      "134 124.38729858398438\n",
      "135 134.78326416015625\n",
      "136 127.98326873779297\n",
      "137 115.42929077148438\n",
      "138 122.5057144165039\n",
      "139 131.00865173339844\n",
      "140 126.194580078125\n",
      "141 124.96459197998047\n",
      "142 129.02622985839844\n",
      "143 129.6086883544922\n",
      "144 130.07606506347656\n",
      "145 125.38990020751953\n",
      "146 119.1093521118164\n",
      "147 135.74676513671875\n",
      "148 122.91841125488281\n",
      "149 136.63063049316406\n",
      "150 134.65924072265625\n",
      "151 126.16912841796875\n",
      "152 126.10081481933594\n",
      "153 124.95887756347656\n",
      "154 132.19003295898438\n",
      "155 129.19158935546875\n",
      "156 124.83255767822266\n",
      "157 119.15896606445312\n",
      "158 127.19508361816406\n",
      "159 120.2138671875\n",
      "160 131.59329223632812\n",
      "161 139.11123657226562\n",
      "162 128.49388122558594\n",
      "163 118.97465515136719\n",
      "164 133.86190795898438\n",
      "165 125.77655792236328\n",
      "166 131.68844604492188\n",
      "167 135.88433837890625\n",
      "168 124.34171295166016\n",
      "169 119.13426208496094\n",
      "170 128.2362823486328\n",
      "171 125.69842529296875\n",
      "172 122.76424407958984\n",
      "173 126.47666931152344\n",
      "174 136.93783569335938\n",
      "175 133.28744506835938\n",
      "176 137.24404907226562\n",
      "177 129.2359619140625\n",
      "178 135.0355682373047\n",
      "179 130.56393432617188\n",
      "180 121.7895736694336\n",
      "181 129.43865966796875\n",
      "182 123.5244140625\n",
      "183 151.13143920898438\n",
      "184 127.58853149414062\n",
      "185 135.03515625\n",
      "186 139.5965576171875\n",
      "187 132.0472869873047\n",
      "188 128.59796142578125\n",
      "189 142.12107849121094\n",
      "190 140.1409149169922\n",
      "191 132.81394958496094\n",
      "192 135.4918212890625\n",
      "193 124.6626968383789\n",
      "194 135.75123596191406\n",
      "195 125.81591796875\n",
      "196 127.2312240600586\n",
      "197 134.43275451660156\n",
      "198 121.84332275390625\n",
      "199 119.87080383300781\n",
      "200 135.23651123046875\n",
      "201 120.60948944091797\n",
      "202 123.14110565185547\n",
      "203 124.49919128417969\n",
      "204 120.69268035888672\n",
      "205 122.87965393066406\n",
      "206 128.48939514160156\n",
      "207 127.48362731933594\n",
      "208 136.0767059326172\n",
      "209 134.084716796875\n",
      "210 127.9998779296875\n",
      "211 118.57154846191406\n",
      "212 126.39334106445312\n",
      "213 129.970703125\n",
      "214 117.65692138671875\n",
      "215 137.52928161621094\n",
      "216 139.33250427246094\n",
      "217 128.61154174804688\n",
      "218 128.18182373046875\n",
      "219 128.68612670898438\n",
      "220 129.36204528808594\n",
      "221 128.00045776367188\n",
      "222 121.86265563964844\n",
      "223 131.62890625\n",
      "224 120.236572265625\n",
      "225 120.14759826660156\n",
      "226 125.55451965332031\n",
      "227 127.3311996459961\n",
      "228 117.85220336914062\n",
      "229 131.43472290039062\n",
      "230 126.36231231689453\n",
      "231 127.04666900634766\n",
      "232 127.44926452636719\n",
      "233 137.34512329101562\n",
      "234 132.00152587890625\n",
      "235 133.72425842285156\n",
      "236 125.57374572753906\n",
      "237 139.26950073242188\n",
      "238 134.8570556640625\n",
      "239 128.75299072265625\n",
      "240 122.67906188964844\n",
      "241 121.30176544189453\n",
      "242 121.98202514648438\n",
      "243 131.360595703125\n",
      "244 127.70491790771484\n",
      "245 120.78938293457031\n",
      "246 124.03718566894531\n",
      "247 135.5771942138672\n",
      "248 124.35787963867188\n",
      "249 134.05743408203125\n",
      "250 121.76322937011719\n",
      "251 124.14122009277344\n",
      "252 136.2069854736328\n",
      "253 134.516357421875\n",
      "254 130.66099548339844\n",
      "255 124.07562255859375\n",
      "256 133.08425903320312\n",
      "257 127.58213806152344\n",
      "258 123.18756866455078\n",
      "259 124.90792846679688\n",
      "260 133.00152587890625\n",
      "261 136.5991668701172\n",
      "262 133.20858764648438\n",
      "263 128.97860717773438\n",
      "264 126.58210754394531\n",
      "265 149.47607421875\n",
      "266 119.82005310058594\n",
      "267 132.82894897460938\n",
      "268 128.51641845703125\n",
      "269 137.35263061523438\n",
      "270 123.1521987915039\n",
      "271 128.29273986816406\n",
      "272 131.99440002441406\n",
      "273 137.54791259765625\n",
      "274 140.28517150878906\n",
      "275 143.23239135742188\n",
      "276 134.1684112548828\n",
      "277 133.97445678710938\n",
      "278 129.96435546875\n",
      "279 135.6851043701172\n",
      "280 131.30697631835938\n",
      "281 125.57295989990234\n",
      "282 123.56367492675781\n",
      "283 129.35325622558594\n",
      "284 138.25439453125\n",
      "285 119.99051666259766\n",
      "286 123.99697875976562\n",
      "287 126.71292114257812\n",
      "288 128.26885986328125\n",
      "289 116.40333557128906\n",
      "290 130.58963012695312\n",
      "291 126.4413833618164\n",
      "292 124.32072448730469\n",
      "293 135.68544006347656\n",
      "294 122.68001556396484\n",
      "295 115.82829284667969\n",
      "296 127.30166625976562\n",
      "297 114.82803344726562\n",
      "298 133.33856201171875\n",
      "299 114.83991241455078\n",
      "300 122.22525024414062\n",
      "301 121.99383544921875\n",
      "302 130.79983520507812\n",
      "303 138.6024169921875\n",
      "304 128.54237365722656\n",
      "305 123.45683288574219\n",
      "306 136.2139129638672\n",
      "307 123.40609741210938\n",
      "308 139.64031982421875\n",
      "309 126.21475219726562\n",
      "310 123.18833923339844\n",
      "311 124.19071197509766\n",
      "312 131.48095703125\n",
      "313 132.94845581054688\n",
      "314 131.4541473388672\n",
      "315 126.76348114013672\n",
      "316 118.82351684570312\n",
      "317 120.75846862792969\n",
      "318 121.9184341430664\n",
      "319 140.0792236328125\n",
      "320 131.5433807373047\n",
      "321 130.76527404785156\n",
      "322 140.27459716796875\n",
      "323 125.68098449707031\n",
      "324 136.80653381347656\n",
      "325 124.12921142578125\n",
      "326 120.63133239746094\n",
      "327 123.8328857421875\n",
      "328 130.242919921875\n",
      "329 140.79083251953125\n",
      "330 127.86184692382812\n",
      "331 131.21339416503906\n",
      "332 133.09976196289062\n",
      "333 132.90467834472656\n",
      "334 123.6201400756836\n",
      "335 126.37477111816406\n",
      "336 149.02520751953125\n",
      "337 128.46180725097656\n",
      "338 132.04110717773438\n",
      "339 128.83326721191406\n",
      "340 141.59555053710938\n",
      "341 137.8255615234375\n",
      "342 128.91412353515625\n",
      "343 120.53135681152344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13432/630850608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0minput_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m680\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         target_lengths = torch.tensor(\n\u001b[1;32m---> 14\u001b[1;33m             [sum([1 for j in i if j > 0]) for i in y_true], dtype=torch.long)\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13432/630850608.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0minput_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m680\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         target_lengths = torch.tensor(\n\u001b[1;32m---> 14\u001b[1;33m             [sum([1 for j in i if j > 0]) for i in y_true], dtype=torch.long)\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13432/630850608.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0minput_lengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m680\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         target_lengths = torch.tensor(\n\u001b[1;32m---> 14\u001b[1;33m             [sum([1 for j in i if j > 0]) for i in y_true], dtype=torch.long)\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for batch_idx, (x, y_true) in enumerate(dataloader_train):\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        y_true = y_true.squeeze(-1)\n",
    "        x, y_true = x.cuda(), y_true.cuda()\n",
    "        logits = model(x).log_softmax(2)\n",
    "        ctc_loss = torch.nn.CTCLoss().cuda()\n",
    "        log_probs = torch.transpose(torch.transpose(\n",
    "            logits, 0, 2), 1, 2).requires_grad_()\n",
    "        targets = y_true\n",
    "        input_lengths = torch.full((batch_size,), 680, dtype=torch.long)\n",
    "        target_lengths = torch.tensor(\n",
    "            [sum([1 for j in i if j > 0]) for i in y_true], dtype=torch.long)\n",
    "        loss = ctc_loss(log_probs, targets, input_lengths, target_lengths)\n",
    "        loss.backward()\n",
    "        print(batch_idx, loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "753b3b938ab180c76750b94459c65ef078bd5597468dfb8aad34d67534b01b6b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pytorch_gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
